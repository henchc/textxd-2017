{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SpaCy](https://spacy.io/): Industrial-Strength NLP\n",
    "\n",
    "The tradtional NLP library has always been [NLTK](http://www.nltk.org/). While `NLTK` is still very useful for linguistics analysis and exporation, `spacy` has become a nice option for easy and fast implementation of the NLP pipeline. What's the NLP pipeline? It's a number of common steps computational linguists perform to help them (and the computer) better understand textual data. Digital Humanists are often fond of the pipeline because it gives us more things to count! Let's what `spacy` can give us that we can count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out with a short string from our reading and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = '''\n",
    "\"What are you going to do with yourself this evening, Alfred?\" said Mr.\n",
    "Royal to his companion, as they issued from his counting-house in New\n",
    "Orleans. \"Perhaps I ought to apologize for not calling you Mr. King,\n",
    "considering the shortness of our acquaintance; but your father and I\n",
    "were like brothers in our youth, and you resemble him so much, I can\n",
    "hardly realize that you are not he himself, and I still a young man.\n",
    "It used to be a joke with us that we must be cousins, since he was a\n",
    "King and I was of the Royal family. So excuse me if I say to you, as\n",
    "I used to say to him. What are you going to do with yourself, Cousin\n",
    "Alfred?\"\n",
    "\n",
    "\"I thank you for the friendly familiarity,\" rejoined the young man.\n",
    "\"It is pleasant to know that I remind you so strongly of my good\n",
    "father. My most earnest wish is to resemble him in character as much\n",
    "as I am said to resemble him in person. I have formed no plans for the\n",
    "evening. I was just about to ask you what there was best worth seeing\n",
    "or hearing in the Crescent City.\"'''.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've downloaded the English model, and now we just have to load it. This model will do ***everything*** for us, but we'll only get a little taste today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# nlp = spacy.load('en', parser=False)  # run this instead if you don't have > 1GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse an entire text we just call the model on a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_text = nlp(my_string)\n",
    "parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick! So what happened? We've talked a lot about tokenizing, either in words or sentences.\n",
    "\n",
    "What about sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sents_tab = Table()\n",
    "sents_tab.append_column(label=\"Sentence\", values=[sentence.text for sentence in parsed_text.sents])\n",
    "sents_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab = Table()\n",
    "toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab.append_column(label=\"Lemma\", values=[word.lemma_ for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else? Let's just make a function `tablefy` that will make a table of all this information for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefy(parsed_text):\n",
    "    toks_tab = Table()\n",
    "    toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Lemma\", values=[word.lemma_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Stop Word\", values=[word.is_stop for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Punctuation\", values=[word.is_punct for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Space\", values=[word.is_space for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Number\", values=[word.like_num for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"OOV\", values=[word.is_oov for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Dependency\", values=[word.dep_ for word in parsed_text])\n",
    "    return toks_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablefy(parsed_text).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "What's the most common verb? Noun? What if you only include lemmata? What if you remove \"stop words\"?\n",
    "\n",
    "How would lemmatizing or removing \"stop words\" help us better understand a text over regular tokenizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dependency Parsing\n",
    "\n",
    "Let's look at our text again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is one of the most useful and interesting NLP tools. A dependency parser will draw a tree of relationships between words. This is how you can find out specifically what adjectives are attributed to a specific person, what verbs are associated with a specific subject, etc.\n",
    "\n",
    "`spacy` provides an online visualizer named \"displaCy\" to visualize dependencies. Let's look at the [first sentence](https://demos.explosion.ai/displacy/?text=%22What%20are%20you%20going%20to%20do%20with%20yourself%20this%20evening%2C%20Alfred%3F%22%20said%20Mr.%20Royal%20to%20his%20companion%2C%20as%20they%20issued%20from%20his%20counting-house%20in%20New%20Orleans.&model=en&cpu=1&cph=1)\n",
    "\n",
    "![alt text](img/dep_parse.png)\n",
    "\n",
    "We can loop through a dependency for a subject by checking the `head` attribute for the `pos` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "SV = []\n",
    "for possible_subject in parsed_text:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        SV.append((possible_subject.text, possible_subject.head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sv_tab = Table()\n",
    "sv_tab.append_column(label=\"Subject\", values=[x[0] for x in SV])\n",
    "sv_tab.append_column(label=\"Verb\", values=[x[1] for x in SV])\n",
    "sv_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that you could look over a large corpus to analyze first person, second person, and third person characterizations. Dependency parsers are also important for understanding and processing natural language, a question answering system for example. These models help the computer understand *what* the question is that is being asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "How accurate are the models? What happens if we change the style of English we're working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = '''\n",
    "Tush! Never tell me; I take it much unkindly\n",
    "That thou, Iago, who hast had my purse\n",
    "As if the strings were thine, shouldst know of this.\n",
    "'''\n",
    "\n",
    "shake_parsed = nlp(shakespeare.strip())\n",
    "tablefy(shake_parsed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huck_finn_jim = '''\n",
    "“Who dah?” “Say, who is you?  Whar is you?  Dog my cats ef I didn’ hear sumf’n.\n",
    "Well, I know what I’s gwyne to do:  I’s gwyne to set down here and listen tell I hears it agin.”\"\n",
    "'''\n",
    "\n",
    "hf_parsed = nlp(huck_finn_jim.strip())\n",
    "tablefy(hf_parsed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_speech = '''\n",
    "LOL where r u rn? omg that's sooo funnnnnny. c u in a sec.\n",
    "'''\n",
    "ts_parsed = nlp(text_speech.strip())\n",
    "tablefy(ts_parsed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_english = '''\n",
    "þæt wearð underne      eorðbuendum, \n",
    "þæt meotod hæfde      miht and strengðo \n",
    "ða he gefestnade      foldan sceatas. \n",
    "'''\n",
    "oe_parsed = nlp(old_english.strip())\n",
    "tablefy(oe_parsed).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER and Civil War-Era Novels\n",
    "\n",
    "Wilkens uses a technique called \"NER\", or \"Named Entity Recognition\" to let the computer identify all of the geographic place names. Wilkens writes:\n",
    "\n",
    "> Text strings representing named locations in the corpus were identified using\n",
    "the named entity recognizer of the Stanford CoreNLP package with supplied training\n",
    "data. To reduce errors and to narrow the results for human review, only those\n",
    "named-location strings that occurred at least five times in the corpus and were used\n",
    "by at least two different authors were accepted. The remaining unique strings were\n",
    "reviewed by hand against their context in each source volume. [883]\n",
    "\n",
    "While we don't have the time for a human review right now, `spacy` does allow us to annotate place names (among other things!) in the same fashion as Stanford CoreNLP (a native Java library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tab = Table()\n",
    "ner_tab.append_column(label=\"NER Label\", values=[ent.label_ for ent in parsed_text.ents])\n",
    "ner_tab.append_column(label=\"NER Text\", values=[ent.text for ent in parsed_text.ents])\n",
    "ner_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! It's identified a few types of things for us. We can check what these mean [here](https://spacy.io/docs/usage/entity-recognition#entity-types). `GPE` is country, cities, or states. Seems like that's what Wilkens was using.\n",
    "\n",
    "Since we don't have his corpus of 1000 novels, let's just take our reading, *A Romance of the Republic*, as an example. We can use the `requests` library to get the raw `HTML` of a web page, and if we take the `.text` property we can make this a nice string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "text = requests.get(\"http://www.gutenberg.org/files/10549/10549.txt\").text\n",
    "text = text[1050:].replace('\\r\\n', ' ')  # fix formatting and skip title header\n",
    "print(text[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leave the chapter headers for now, it shouldn't affect much. Now we need to parse this with that `nlp` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "With this larger string, find the most common noun, verb, and adjective. Then explore the other features of `spacy` and see what you can discover about our reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's continue in the fashion that Wilkens did and extract the named entities, specifically those for \"GPE\". We can loop through each entity, and if it is labeled as `GPE` we'll add it to our `places` list. We'll then make a `Counter` object out of that to get the frequency of each place name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "places = []\n",
    "\n",
    "for ent in parsed.ents:\n",
    "    if ent.label_ == \"GPE\":\n",
    "        places.append(ent.text.strip())\n",
    "\n",
    "places = Counter(places)\n",
    "places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks OK, but it's pretty rough! Keep this in mind when using trained models. They aren't 100% accurate. That's why Wilkens went through by hand after to get rid of the garbage.\n",
    "\n",
    "If you thought NER was cool, wait for this. Now that we have a list of \"places\", we can send that to an online database to get back latitude and longitude coordinates (much like Wilkens used Google's geocoder), along with the US state. To make sure it's actually a US state, we'll need a list to compare to. So let's load that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/us_states.txt', 'r') as f:\n",
    "    states = f.read().split('\\n')\n",
    "    states = [x.strip() for x in states]\n",
    "\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we're ready. The `Nominatim` function from the `geopy` library will return an object that has the properties we want. We'll append a new row to our table for each entry. Importantly, we're using the `keys` of the `places` counter because we don't need to ask the database for \"New Orleans\" 10 times to get the location. So after we get the information we'll just add as many rows as the counter tells us there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from datascience import *\n",
    "import time\n",
    "\n",
    "geolocator = Nominatim(timeout=10)\n",
    "\n",
    "geo_tab = Table([\"latitude\", \"longitude\", \"name\", \"state\"])\n",
    "\n",
    "for name in places.keys():  # only want to loop through unique place names to call once per place name\n",
    "    print(\"Getting information for \" + name + \"...\")\n",
    "    \n",
    "    # finds the lat and lon of each name in the locations list\n",
    "    location = geolocator.geocode(name)\n",
    "\n",
    "    try:\n",
    "        # index the raw response for lat and lon\n",
    "        lat = float(location.raw[\"lat\"])\n",
    "        lon = float(location.raw[\"lon\"])\n",
    "        \n",
    "        # string manipulation to find state name\n",
    "        for p in location.address.split(\",\"):\n",
    "            if p.strip() in states:\n",
    "                state = p.strip()\n",
    "                break\n",
    "\n",
    "        # add to our table\n",
    "        for i in range(places[name] - 1):\n",
    "            geo_tab.append(Table.from_records([{\"name\": name,\n",
    "                                          \"latitude\": lat,\n",
    "                                          \"longitude\": lon,\n",
    "                                          \"state\": state}]).row(0))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot a nice [choropleth](https://en.wikipedia.org/wiki/Choropleth_map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scripts.choropleth import us_choropleth\n",
    "us_choropleth(geo_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Homework:\n",
    "\n",
    "Find the text to three different Civil War-Era (1851-1875) novels on [Project Gutenberg](https://www.gutenberg.org/) (maybe mentioned in our reading?!). Make sure you click for the `.txt` files, and use a `GET` request from the `requests` library to get the text. \n",
    "\n",
    "First do some exploration on parts of speech. Then combine the NER location frequencies and plot a choropleth. Look closely at the words plotted. How did the NER model do? How does your choropleth look compared to Wilkens'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
