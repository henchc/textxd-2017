{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../share/Aerial_view_LLNL.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 15px ; height: 250px; background-position: 0% 80%'>\n",
    "    <div style=\"float: center ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.8) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.9) ; line-height: 100%\">Notebook 5:</div>\n",
    "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Part of Speech Tagging and Named Entity Recognition</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Estimated Time: 30 minutes</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SpaCy](https://spacy.io/): Industrial-Strength NLP\n",
    "\n",
    "The tradtional NLP library has always been [NLTK](http://www.nltk.org/). While `NLTK` is still very useful for linguistics analysis and exporation, `spacy` has become a nice option for easy and fast implementation of the NLP pipeline. What's the NLP pipeline? It's a number of common steps computational linguists perform to help them (and the computer) better understand textual data. Digital Humanists are often fond of the pipeline because it gives us more things to count! Let's what `spacy` can give us that we can count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out with a short string from [Lydia Maria Child](https://en.wikipedia.org/wiki/Lydia_Maria_Child)'s *Romance of the Republic* and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = '''\n",
    "\"What are you going to do with yourself this evening, Alfred?\" said Mr.\n",
    "Royal to his companion, as they issued from his counting-house in New\n",
    "Orleans. \"Perhaps I ought to apologize for not calling you Mr. King,\n",
    "considering the shortness of our acquaintance; but your father and I\n",
    "were like brothers in our youth, and you resemble him so much, I can\n",
    "hardly realize that you are not he himself, and I still a young man.\n",
    "It used to be a joke with us that we must be cousins, since he was a\n",
    "King and I was of the Royal family. So excuse me if I say to you, as\n",
    "I used to say to him. What are you going to do with yourself, Cousin\n",
    "Alfred?\"\n",
    "\n",
    "\"I thank you for the friendly familiarity,\" rejoined the young man.\n",
    "\"It is pleasant to know that I remind you so strongly of my good\n",
    "father. My most earnest wish is to resemble him in character as much\n",
    "as I am said to resemble him in person. I have formed no plans for the\n",
    "evening. I was just about to ask you what there was best worth seeing\n",
    "or hearing in the Crescent City.\"'''.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've downloaded the English model, and now we just have to load it. This model will do ***everything*** for us, but we'll only get a little taste today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en', parser=False)  # run this instead if you don't have > 1GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse an entire text we just call the model on a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_text = nlp(my_string)\n",
    "parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick! So what happened? We've talked a lot about tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.text for word in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.pos_ for word in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.lemma_ for word in parsed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else? Let's just make a function `tablefy` that will make a table of all this information for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablefy(parsed_text):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Word\"] = [word.text for word in parsed_text]\n",
    "    df[\"POS\"] = [word.pos_ for word in parsed_text]\n",
    "    df[\"Lemma\"] = [word.lemma_ for word in parsed_text]\n",
    "    df[\"Stop Word\"] = [word.is_stop for word in parsed_text]\n",
    "    df[\"Punctuation\"] = [word.is_punct for word in parsed_text]\n",
    "    df[\"Space\"] = [word.is_space for word in parsed_text]\n",
    "    df[\"Number\"] = [word.like_num for word in parsed_text]\n",
    "    df[\"OOV\"] = [word.is_oov for word in parsed_text]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tablefy(parsed_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it in a table format, we can use `pandas` to do some subsetting and counting. `pandas` is the most popular data analysis library for Python. While it's syntax may be confusing at first, it's worth getting to know!\n",
    "\n",
    "**Subsetting**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['POS'] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['POS'] == 'NOUN') & (df['Stop Word'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['POS'] == 'NOUN') | (df['POS'] == 'VERB')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting**\n",
    "\n",
    "Note that when we index a `DataFrame`, we get back a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(df['Word']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df[df['POS'] == 'NOUN']['Word']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "What's the most common verb? Noun? What if you only include lemmata? What if you remove \"stop words\"?\n",
    "\n",
    "How would lemmatizing or removing \"stop words\" help us better understand a text over regular tokenizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "How accurate are the models? What happens if we change the style of English we're working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = '''\n",
    "Tush! Never tell me; I take it much unkindly\n",
    "That thou, Iago, who hast had my purse\n",
    "As if the strings were thine, shouldst know of this.\n",
    "'''\n",
    "\n",
    "shake_parsed = nlp(shakespeare.strip())\n",
    "tablefy(shake_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huck_finn_jim = '''\n",
    "“Who dah?” “Say, who is you?  Whar is you?  Dog my cats ef I didn’ hear sumf’n.\n",
    "Well, I know what I’s gwyne to do:  I’s gwyne to set down here and listen tell I hears it agin.”\"\n",
    "'''\n",
    "\n",
    "hf_parsed = nlp(huck_finn_jim.strip())\n",
    "tablefy(hf_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_speech = '''\n",
    "LOL where r u rn? omg that's sooo funnnnnny. c u in a sec.\n",
    "'''\n",
    "ts_parsed = nlp(text_speech.strip())\n",
    "tablefy(ts_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_english = '''\n",
    "þæt wearð underne      eorðbuendum, \n",
    "þæt meotod hæfde      miht and strengðo \n",
    "ða he gefestnade      foldan sceatas. \n",
    "'''\n",
    "oe_parsed = nlp(old_english.strip())\n",
    "tablefy(oe_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = pd.DataFrame()\n",
    "ner_df['entity_type'] = [ent.label_ for ent in parsed_text.ents]\n",
    "ner_df['text'] = [ent.text for ent in parsed_text.ents]\n",
    "ner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! It's identified a few types of things for us. We can check what these mean [here](https://spacy.io/docs/usage/entity-recognition#entity-types). `GPE` is country, cities, or states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's subset these geographic locations ('GPE'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_names = ner_df[ner_df['entity_type'] == 'GPE']['text']\n",
    "place_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Nominatim` function from the `geopy` library will return an object that has latitude and longitude. Let's take the place names and map them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "geolocator = Nominatim(timeout=10)\n",
    "place_names_coords = []\n",
    "\n",
    "for name in place_names:  # only want to loop through unique place names to call once per place name\n",
    "    print(\"Getting information for \" + name + \"...\")\n",
    "    \n",
    "    # finds the lat and lon of each name in the locations list\n",
    "    location = geolocator.geocode(name)\n",
    "\n",
    "    # index the raw response for lat and lon\n",
    "    lat = float(location.raw[\"lat\"])\n",
    "    lon = float(location.raw[\"lon\"])\n",
    "    print(lat, lon)\n",
    "    place_names_coords.append((name, (lat, lon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have our place names in `place_names_coords`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_names_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `folium` mapping library now to put these dots on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import IFrame\n",
    "\n",
    "map = folium.Map(location=[39.8333333,-98.585522], zoom_start=3)\n",
    "\n",
    "for l in place_names_coords:\n",
    "\n",
    "    folium.CircleMarker((l[1][0], l[1][1]),\n",
    "                radius=1,\n",
    "                popup=l[0],\n",
    "                color=\"blue\"\n",
    "               ).add_to(map)\n",
    "\n",
    "map.save(\"map.html\")\n",
    "IFrame('map.html', width=700, height=400)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
