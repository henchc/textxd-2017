{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../share/Aerial_view_LLNL.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 15px ; height: 250px; background-position: 0% 80%'>\n",
    "    <div style=\"float: center ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.8) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.9) ; line-height: 100%\">Notebook 6:</div>\n",
    "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Document Term Matrix and Textual Similarity</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Estimated Time: 45 minutes</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Similarity\n",
    "\n",
    "## Bag of Words (BoW) language model\n",
    "\n",
    "Today we'll see our first, admittedly primitive, computational model of language called \"Bag of Words\". This model was very popular in early text analysis, and continues to be used today. In fact, the models that have replaced it are still very difficult to actually interpret, giving the BoW approach a slight advantage if we want to understand why the model makes certain decisions.\n",
    "\n",
    "Getting into the model we'll have to revisit Term Frequency (think `Counter`). We'll then see the Document-Term Matrix (DTM), which we've discusssed briefly before. We'll have to normalize these counts if we want to compare. Then we'll look at the available Python libraries to streamline this process.\n",
    "\n",
    "Once we have our BoW model we can analyze it in a high-dimensional vector space, which gives us more insights into the similarities and clustering of different texts.\n",
    "\n",
    "Let's read in Augustine's *Confessions* text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Augustine-Confessions.txt') as f:\n",
    "    confessions = f.read()\n",
    "\n",
    "print(confessions[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 13 books, which are fortunately separated by six line breaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confessions_list = confessions.split('\\n'*6)\n",
    "len(confessions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confessions_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency Revisited\n",
    "\n",
    "We'll remember from last week, that while `split` might be a quick way to get tokens, it's not the most accurate because it doesn't separate punctuation and contractions. We'll use `spacy` again to get tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', parser=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_book = confessions_list[0]\n",
    "parsed = nlp(first_book)\n",
    "first_token_list = [token.text for token in parsed]\n",
    "first_token_list[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `Counter` to get the term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_freq = Counter(first_token_list)\n",
    "word_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Write some code to get the 20 most common words of the second book. How similar are they to those of the first book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plan to compare word frequencies across texts, we could collate these `Counter` dictionaries for each book in `Confessions`. But we don't want to write all that code! There is an easy function that streamlines the process called `CountVectorizer`.\n",
    "\n",
    "Let's look at the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So we'll create the `CountVectorizer` object, then transform it on our `list` of documents, here that would be the books in Augustine's `Confessions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "dtm = cv.fit_transform(confessions_list)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? A sparse matrix just means that some cells in the table don't have value. Why? Because the vocabulary base is not the same for all the books! Let's try to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# de-sparsify\n",
    "desparse = dtm.toarray()\n",
    "\n",
    "# create labels for columns\n",
    "word_list = cv.get_feature_names()\n",
    "\n",
    "# create a new table\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=desparse)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the ***Document Term Matrix***. This is a core concept in NLP and text analysis. It's not that complicated!\n",
    "\n",
    "We have columns for each word *in the entire corpus*. Then each *row* is for each *document*. In our case, that's books in *Confessions*. The values are the word count for that word in the corresponding document. Note that there are many 0s, that word just doesn't show up in that document!\n",
    "\n",
    "We can call up frequencies for a given word for each chapter easily, since they are the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df['read']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks to be about 13 counts, one for each book, let's double check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dtm_df['read'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this another step further. In order to make apples-to-apples comparisons across Books, we can normalize our values by dividing each word count by the total number of words in its Book. To do that, we'll need to `sum` on `axis=1`, which means summing the row (number of words in that book), as opposed to summing the column.\n",
    "\n",
    "Once we have the total number of words in that Book, we can get the percentage of words that one particular word accounts for, and we can do that for every word across the matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "row_sums = np.sum(desparse, axis=1)\n",
    "normed = desparse/row_sums[:,None]\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=normed)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the matrix above, we see that the word \"abandoned\" accounts for .0147% of words in Book 1, and .0278% of words in Book 2.\n",
    "\n",
    "We can still grab out the normalized frequencies of the word 'read' for each book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df['abandoned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a variety of reasons we like to remove words like \"the\", \"of\", \"and\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using an older translation of Augustine, we have to remove archaic forms of these stopwords as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye_olde_stop_words = ['thou','thy','thee', 'thine', 'ye', 'hath','hast', 'wilt','aught',\\\n",
    "                      'art', 'dost','doth', 'shall', 'shalt','tis','canst','thyself',\\\n",
    "                     'didst', 'yea', 'wert']\n",
    "\n",
    "stop_words = list(ENGLISH_STOP_WORDS) + ye_olde_stop_words\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run our code above, but feed in the `stop_words` to the `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "dtm = cv.fit_transform(confessions_list)\n",
    "desparse = dtm.toarray()\n",
    "word_list = cv.get_feature_names()\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=desparse)\n",
    "row_sums = np.sum(desparse, axis=1)\n",
    "normed = desparse/row_sums[:,None]\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=normed)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of work, if this is such a common task hasn't someone streamlined this? In fact, we can simply instruct `CountVectorizer` not to include stopwords at all and another function, `TfidfTransformer`, normalizes easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "dtm = cv.fit_transform(confessions_list)\n",
    "tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "dtm_tf = tt.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model of Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have a matrix with normalized frequencies of all the words ***in the entire corpus***. Right now our corpus is just all the books in Augustine's *Confessions*.\n",
    "\n",
    "Let's move away from the table and just create a list of 13 vectors with only the normalized frequency values, one for each Book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_array = dtm_tf.toarray()\n",
    "dtm_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector has a number of coordinates equal to the number of unique words in the corpus. Let's just take Book 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtm_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to measure the similarity of texts, which Piper uses in his article, would be to measure the *Euclidean distance* between their coordinates in space. According to Wikipedia:\n",
    "\n",
    ">The Euclidean distance or Euclidean metric is the \"ordinary\" straight-line distance between two points in Euclidean space\n",
    "\n",
    ">$\\mathrm{d}(\\mathbf{b},\\mathbf{a})=\\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2}$\n",
    "\n",
    "Let's consider a simple 2 dimensional model. We have two point in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2,6)\n",
    "b = (5,10)\n",
    "\n",
    "euc_dist = np.sqrt( (a[0]-b[0])**2  +  (a[1]-b[1])**2 )\n",
    "euc_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter([a[0], b[0]], [a[1], b[1]])\n",
    "plt.plot([a[0], b[0]], [a[1], b[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of this 2 dimensional distance between 2 points as looking at 2 different texts. In this *very* simple 2-d model though, we only have 2 words in the entire corpus! `(2,6)` and `(5,10)` would be the absolute counts for each text. Imagine:\n",
    "\n",
    "```\n",
    "Document 1:\n",
    "\n",
    "the dog the dog dog dog dog dog\n",
    "\n",
    "Document 2:\n",
    "\n",
    "the dog the dog the dog the dog the dog dog dog dog dog dog\n",
    "\n",
    "```\n",
    "\n",
    "That would yield the comparison above. If we added a third point (document), we could see which 2 documents were closest to one another!\n",
    "\n",
    "---\n",
    "\n",
    "Ok, not too bad, but how do we do this with hundreds or thousands of dimensions (words) acorss hundreds or thousands of points (documents)? Well it actually scales the same way! Here it is for 3 dimensions:\n",
    "\n",
    "$\\mathrm{d}(\\mathbf{b},\\mathbf{a})=\\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + (a_3-b_3)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2,6,15)\n",
    "b = (5,10,3)\n",
    "\n",
    "euc_dist = np.sqrt( (a[0]-b[0])**2 +  (a[1]-b[1])**2 + (a[2]-b[2])**2 )\n",
    "euc_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter([a[0], b[0]], [a[1], b[1]], [a[2], b[2]])\n",
    "ax.plot([a[0], b[0]], [a[1], b[1]], [a[2], b[2]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to use our cool formula to calculate this, or to scale it up for *n* dimensions. That's what `scipy` is for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "distance.euclidean(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another measure of two vectors, more common for text analysis, is called *cosine similarity*. According to Wikipedia:\n",
    "\n",
    ">Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    ">$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\|_2 \\|\\mathbf{B}\\|_2} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$\n",
    "\n",
    "Essentially we want to take the cosine of the angle formed between two vectors (documents). We start the vector at the origin and measure the angle between the two vectors we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "origin = (0,0,0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter([a[0], b[0], origin[0]], [a[1], b[1], origin[1]], [a[2], b[2], origin[2]])\n",
    "ax.plot([origin[0], a[0]], [origin[1], a[1]], [origin[2], a[2]])\n",
    "ax.plot([origin[0], b[0]], [origin[1], b[1]], [origin[2], b[2]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to two dimensions for the vanilla `numpy` calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2,6)\n",
    "b = (5,10)\n",
    "\n",
    "# don't worry about the formula so much as the intuition behind it: angle between vectors\n",
    "cos_dist = 1 - (a[0]*b[0] + a[1]*b[1]) / ( np.sqrt(a[0]**2 + a[1]**2 ) * np.sqrt(b[0]**2 + b[1]**2 ) )\n",
    "cos_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, `scipy` has taken care of this for us too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.cosine(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 3-d model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2,6,15)\n",
    "b = (5,10,3)\n",
    "distance.cosine(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try passing different values into both the euclidean and cosine distance functions. What is your intuition about these different measurements? Remember that all values in the Term-Frequency Matrix are positive, between [0,1], and that most are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Texts in Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this now. Say we have 3 texts, `a`, `b`, and `c`. The whole corpus, again, only has 2 words (dimensions)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2,6)\n",
    "b = (5,10)\n",
    "c = (14,11)\n",
    "\n",
    "print(distance.euclidean(a,b))\n",
    "print(distance.euclidean(a,c))\n",
    "print(distance.euclidean(b,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a matrix for the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "point_matrix = np.array([a,b,c])\n",
    "point_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `sklearn`'s `pairwise_distances` method to compare each book to each book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "pairwise.pairwise_distances(point_matrix, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We got what we calculated. Note: the results are mirrored because the columns and rows are both the same texts.\n",
    "\n",
    "We can do the same thing on Augustine's *Confessions*, remember the rows are for each Book too!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = pairwise.pairwise_distances(dtm_tf, metric='euclidean')\n",
    "\n",
    "title_list = ['Book '+str(i+1) for i in range(len(confessions_list))]\n",
    "pd.DataFrame(columns=title_list, data=dist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing hundreds of dimensions is difficult for us. So we can use multi-dimensional scaling (MDS) to put this into a 2-d graph for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components = 2, dissimilarity=\"precomputed\")\n",
    "embeddings = mds.fit_transform(dist_matrix)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(13):\n",
    "    ax.annotate(i+1, ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Aside: K-Means Clustering\n",
    "\n",
    "Tries to find natural groupings among points, once we tell it how many groups to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit_predict(dist_matrix)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
